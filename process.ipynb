{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      series_id  step     anglez    enmo  awake  hour_sin  hour_cos\n",
      "0  08db4255286f     0 -30.845301  0.0447      1       0.5 -0.866025\n",
      "1  08db4255286f     1 -34.181801  0.0443      1       0.5 -0.866025\n",
      "2  08db4255286f     2 -33.877102  0.0483      1       0.5 -0.866025\n",
      "3  08db4255286f     3 -34.282101  0.0680      1       0.5 -0.866025\n",
      "4  08db4255286f     4 -34.385799  0.0768      1       0.5 -0.866025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# üìå Charger et transformer le dataset\n",
    "train = (pl.scan_parquet(\"./Data/Zzzs_train_multi.parquet\")\n",
    "          .with_columns(\n",
    "              (pl.col(\"timestamp\")\n",
    "                 .str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S%Z\")\n",
    "                 .dt.hour()\n",
    "                 .alias(\"hour\")  # Extraire l'heure\n",
    "              )\n",
    "          )\n",
    "          .drop(\"timestamp\")  # Supprimer timestamp original\n",
    "          .collect()\n",
    "          .to_pandas()\n",
    ")\n",
    "\n",
    "# üìå Ajouter encodage sinuso√Ødal du temps\n",
    "train['hour_sin'] = np.sin(2 * np.pi * train['hour'] / 24)\n",
    "train['hour_cos'] = np.cos(2 * np.pi * train['hour'] / 24)\n",
    "train = train.drop(columns=['hour'])  # Supprimer la colonne brute\n",
    "df = train.copy()\n",
    "# üìå Affichage des premi√®res lignes\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      series_id  step     anglez    enmo  awake  hour_sin  hour_cos\n",
      "0  08db4255286f     0 -30.845301  0.0447      1       0.5 -0.866025\n",
      "1  08db4255286f     1 -34.181801  0.0443      1       0.5 -0.866025\n",
      "2  08db4255286f     2 -33.877102  0.0483      1       0.5 -0.866025\n",
      "3  08db4255286f     3 -34.282101  0.0680      1       0.5 -0.866025\n",
      "4  08db4255286f     4 -34.385799  0.0768      1       0.5 -0.866025\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "series_id    0\n",
      "step         0\n",
      "anglez       0\n",
      "enmo         0\n",
      "awake        0\n",
      "hour_sin     0\n",
      "hour_cos     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Nombre de Nan par colonne\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les variances de anglez et enmo sur une fen√™tre de 5 pas\n",
    "\n",
    "df['anglez_var'] = df['anglez'].rolling(window=5, min_periods=1).var().fillna(0)\n",
    "df['enmo_var'] = df['enmo'].rolling(window=5, min_periods=1).var().fillna(0)\n",
    "\n",
    "\n",
    "# S√©lection des features et de la cible\n",
    "features = ['hour_cos','hour_sin','anglez', 'enmo', 'anglez_var', 'enmo_var']\n",
    "X = df[features]\n",
    "y = df['awake']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [series_id, step, anglez, enmo, awake, hour_sin, hour_cos, anglez_var, enmo_var]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# lignes contenants des Nan\n",
    "print(df[df.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©gression Logistique:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78    900864\n",
      "           1       0.76      0.92      0.83   1731984\n",
      "           2       0.24      0.01      0.03    625116\n",
      "\n",
      "    accuracy                           0.73   3257964\n",
      "   macro avg       0.56      0.60      0.55   3257964\n",
      "weighted avg       0.64      0.73      0.66   3257964\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:12:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91    900864\n",
      "           1       0.83      0.91      0.87   1731984\n",
      "           2       0.78      0.49      0.60    625116\n",
      "\n",
      "    accuracy                           0.84   3257964\n",
      "   macro avg       0.83      0.78      0.79   3257964\n",
      "weighted avg       0.83      0.84      0.83   3257964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardisation des features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# S√©paration des donn√©es en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Mod√®le R√©gression Logistique\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "print(\"R√©gression Logistique:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "# Mod√®le XGBoost\n",
    "xgb = XGBClassifier(scale_pos_weight=212, eval_metric='logloss')\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "print(\"XGBoost:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible : True\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:21:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:21:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\", \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs param√®tres : {'subsample': 1.0, 'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1, 'gamma': 0.3, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:21:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:21:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91    900864\n",
      "           1       0.83      0.91      0.87   1731984\n",
      "           2       0.78      0.49      0.60    625116\n",
      "\n",
      "    accuracy                           0.84   3257964\n",
      "   macro avg       0.83      0.78      0.79   3257964\n",
      "weighted avg       0.83      0.84      0.83   3257964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# V√©rifier si GPU dispo\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"GPU disponible :\", use_gpu)\n",
    "\n",
    "# D√©finition des hyperparam√®tres √† tester\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  # Nombre d'arbres\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Taux d'apprentissage\n",
    "    'max_depth': [3, 6, 9],  # Profondeur des arbres\n",
    "    'subsample': [0.7, 0.8, 1.0],  # Pourcentage de donn√©es utilis√©es par arbre\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],  # Pourcentage de colonnes utilis√©es par arbre\n",
    "    'gamma': [0, 0.1, 0.3],  # R√©duction de sur-apprentissage\n",
    "}\n",
    "\n",
    "# Initialisation du mod√®le avec GPU\n",
    "xgb = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=5,  # Gestion du d√©s√©quilibre\n",
    "    tree_method=\"gpu_hist\" if use_gpu else \"hist\",  # Utiliser GPU si dispo\n",
    "    predictor=\"gpu_predictor\" if use_gpu else \"cpu_predictor\"\n",
    ")\n",
    "\n",
    "# Randomized Search pour trouver les meilleurs param√®tres\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb, param_distributions=param_grid,\n",
    "    n_iter=5, cv=3, scoring='f1_weighted', verbose=2, n_jobs=4  # Moins de charge CPU\n",
    ")\n",
    "\n",
    "# Entra√Ænement sur un sous-ensemble des donn√©es pour r√©duire la RAM utilis√©e\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(\n",
    "    X_train, y_train, train_size=0.5, stratify=y_train, random_state=42\n",
    ")\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Affichage des meilleurs param√®tres\n",
    "print(\"Meilleurs param√®tres :\", random_search.best_params_)\n",
    "\n",
    "# Test sur les donn√©es de validation\n",
    "best_xgb = random_search.best_estimator_\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meilleurs param√®tres : {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:21:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:21:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:23:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8630110707177857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93    900864\n",
      "           1       0.85      0.92      0.88   1731984\n",
      "           2       0.82      0.58      0.68    625116\n",
      "\n",
      "    accuracy                           0.86   3257964\n",
      "   macro avg       0.86      0.82      0.83   3257964\n",
      "weighted avg       0.86      0.86      0.86   3257964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Meilleurs param√®tres\n",
    "best_params = {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 1.0}\n",
    "# Initialisation du mod√®le avec les meilleurs param√®tres\n",
    "xgb = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=5,  # Gestion du\n",
    "    tree_method=\"gpu_hist\" if use_gpu else \"hist\",  # Utiliser GPU si dispo\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "# Entra√Ænement sur toutes les donn√©es\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "# Affichage de l'accuracy\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "# Affichage du rapport de classification\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset Zzzs_train.parquet\n",
    "df2 = (pl.scan_parquet(\"./Data/Zzzs_train.parquet\")\n",
    "          .with_columns(\n",
    "              (pl.col(\"timestamp\")\n",
    "                 .str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S%Z\")\n",
    "                 .dt.hour()\n",
    "                 .alias(\"hour\")  # Extraire l'heure\n",
    "              )\n",
    "          )\n",
    "          .drop(\"timestamp\")  # Supprimer timestamp original\n",
    "          .collect()\n",
    "          .to_pandas()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['hour_sin'] = np.sin(2 * np.pi * df2['hour'] / 24)\n",
    "df2['hour_cos'] = np.cos(2 * np.pi * df2['hour'] / 24)\n",
    "df2 = df2.drop(columns=['hour'])  # Supprimer la colonne brute\n",
    "\n",
    "# Ajouter les variances de anglez et enmo sur une fen√™tre de 5 pas\n",
    "df2['anglez_var'] = df2['anglez'].rolling(window=5, min_periods=1).var().fillna(0)\n",
    "df2['enmo_var'] = df2['enmo'].rolling(window=5, min_periods=1).var().fillna(0)\n",
    "\n",
    "# S√©lection des features\n",
    "features = ['hour_cos', 'hour_sin', 'anglez', 'enmo', 'anglez_var', 'enmo_var']\n",
    "X = df2[features]\n",
    "y = df2['awake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# S√©paration des donn√©es en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible : True\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:25:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:25:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs param√®tres : {'subsample': 1.0, 'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:26:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.91    900753\n",
      "           1       0.93      0.98      0.96   1732359\n",
      "\n",
      "    accuracy                           0.94   2633112\n",
      "   macro avg       0.95      0.92      0.94   2633112\n",
      "weighted avg       0.94      0.94      0.94   2633112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# V√©rifier si GPU dispo\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"GPU disponible :\", use_gpu)\n",
    "\n",
    "# D√©finition des hyperparam√®tres √† tester\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  # Nombre d'arbres\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Taux d'apprentissage\n",
    "    'max_depth': [3, 6, 9],  # Profondeur des arbres\n",
    "    'subsample': [0.7, 0.8, 1.0],  # Pourcentage de donn√©es utilis√©es par arbre\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],  # Pourcentage de colonnes utilis√©es par arbre\n",
    "    'gamma': [0, 0.1, 0.3],  # R√©duction de sur-apprentissage\n",
    "}\n",
    "\n",
    "# Initialisation du mod√®le avec GPU\n",
    "xgb = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=5,  # Gestion du d√©s√©quilibre\n",
    "    tree_method=\"gpu_hist\" if use_gpu else \"hist\",  # Utiliser GPU si dispo\n",
    "    predictor=\"gpu_predictor\" if use_gpu else \"cpu_predictor\"\n",
    ")\n",
    "\n",
    "# Randomized Search pour trouver les meilleurs param√®tres\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb, param_distributions=param_grid,\n",
    "    n_iter=5, cv=3, scoring='f1_weighted', verbose=2, n_jobs=4  # Moins de charge CPU\n",
    ")\n",
    "\n",
    "# Entra√Ænement sur un sous-ensemble des donn√©es pour r√©duire la RAM utilis√©e\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(\n",
    "    X_train, y_train, train_size=0.5, stratify=y_train, random_state=42\n",
    ")\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Affichage des meilleurs param√®tres\n",
    "print(\"Meilleurs param√®tres :\", random_search.best_params_)\n",
    "\n",
    "# Test sur les donn√©es de validation\n",
    "best_xgb = random_search.best_estimator_\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meilleurs param√®tres : {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:07:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs param√®tres : {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 500, 'subsample': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leami\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:08:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93    900753\n",
      "           1       0.97      0.96      0.96   1732359\n",
      "\n",
      "    accuracy                           0.95   2633112\n",
      "   macro avg       0.95      0.95      0.95   2633112\n",
      "weighted avg       0.95      0.95      0.95   2633112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EN grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# D√©finition des hyperparam√®tres √† tester\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500],  \n",
    "    'learning_rate': [0.01,0.1],  \n",
    "    'max_depth': [6],  \n",
    "    'subsample': [0.7],  \n",
    "    'colsample_bytree': [0.8],  \n",
    "    'gamma': [0],  \n",
    "}\n",
    "\n",
    "\n",
    "# Initialisation du mod√®le avec GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "xgb = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    tree_method=\"gpu_hist\" if use_gpu else \"hist\"\n",
    ")\n",
    "\n",
    "# Grid Search pour trouver les meilleurs param√®tres\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb, param_grid=param_grid,\n",
    "    cv=3, scoring='f1_weighted', verbose=2, n_jobs=4  # Moins de charge CPU\n",
    ")\n",
    "\n",
    "# Entra√Ænement sur toutes les donn√©es\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs param√®tres\n",
    "print(\"Meilleurs param√®tres :\", grid_search.best_params_)\n",
    "# Test sur les donn√©es de validation\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meilleurs param√®tres : {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 500, 'subsample': 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train : (130297, 100, 4)\n",
      "Shape y_train : (130297,)\n"
     ]
    }
   ],
   "source": [
    "# üìå S√©lection des features et target\n",
    "features = ['anglez', 'enmo', 'hour_sin', 'hour_cos']\n",
    "target = 'awake'\n",
    "\n",
    "def create_sequences(df, seq_len=10):\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for series in df['series_id'].unique():  \n",
    "        subset = df[df['series_id'] == series]\n",
    "        X_values = subset[features].values\n",
    "        y_values = subset[target].values\n",
    "        \n",
    "        for i in range(0, len(X_values) - seq_len, seq_len):  \n",
    "            X_seq.append(X_values[i:i+seq_len])\n",
    "            y_seq.append(y_values[i+seq_len-1]) \n",
    "        \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X, Y = create_sequences(train, seq_len=100)\n",
    "\n",
    "# üìå Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape X_train :\", X_train.shape)\n",
    "print(\"Shape y_train :\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# üìå Cr√©ation d'un dataset PyTorch\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# üìå Convertir les donn√©es en datasets PyTorch\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# üìå DataLoader pour batch processing\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Activation Softmax pour classification multi-classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Derni√®re sortie de la s√©quence\n",
    "        return self.softmax(out)  # Activation softmax pour pr√©diction multi-classes\n",
    "\n",
    "# üìå Initialisation du mod√®le\n",
    "input_size = X_train.shape[2]  # Nombre de features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 3  # üü¢ Maintenant 3 classes (0,1,2)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # üî• CrossEntropy pour classification multi-classes\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.19567152724481965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.00      0.00      8938\n",
      "         1.0       0.43      0.00      0.01     17306\n",
      "         2.0       0.19      1.00      0.33      6331\n",
      "\n",
      "    accuracy                           0.20     32575\n",
      "   macro avg       0.50      0.33      0.11     32575\n",
      "weighted avg       0.51      0.20      0.07     32575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model.eval()\n",
    "y_pred_list, y_true_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        y_pred = torch.argmax(outputs, dim=1).cpu().numpy()  # üî• Prendre la classe avec la plus haute proba\n",
    "        \n",
    "        y_pred_list.extend(y_pred)\n",
    "        y_true_list.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# üìå Affichage des performances\n",
    "print(\"Accuracy:\", accuracy_score(y_true_list, y_pred_list))\n",
    "print(classification_report(y_true_list, y_pred_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
